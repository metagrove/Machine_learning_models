# -*- coding: utf-8 -*-
"""Transformer-From-scratch

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tbdg7snlLZtHNO_UKaMysNgUpswFcytB
"""

import torch
import torch.nn as nn
import math

class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size, embed_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)

    def forward(self, x):
        return self.embedding(x)

class PositionalEncoding(nn.Module):
    def __init__(self, embed_size, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, embed_size)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_size, num_heads):
        super().__init__()
        assert embed_size % num_heads == 0, "Embed size must be divisible by number of heads"

        self.embed_size = embed_size
        self.num_heads = num_heads
        self.head_dim = embed_size // num_heads

        self.query = nn.Linear(embed_size, embed_size)
        self.key = nn.Linear(embed_size, embed_size)
        self.value = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)

    def forward(self, x, mask=None):
        N, seq_length, embed_size = x.shape

        # Linear projections
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)

        # Reshape: [N, seq_length, num_heads, head_dim] â†’ [N, num_heads, seq_length, head_dim]
        def split_heads(tensor):
            return tensor.view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)

        Q = split_heads(Q)
        K = split_heads(K)
        V = split_heads(V)

        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attention = torch.softmax(scores, dim=-1)
        out = torch.matmul(attention, V)  # shape: [N, num_heads, seq_length, head_dim]

        # Concatenate heads
        out = out.transpose(1, 2).contiguous().view(N, seq_length, self.embed_size)

        # Final linear layer
        out = self.fc_out(out)
        return out

if __name__ == "__main__":
    attn = MultiHeadSelfAttention(embed_size=512, num_heads=8)
    x = torch.rand(2, 10, 512)  # Batch of 2 sequences, each 10 tokens long
    out = attn(x)
    print(out.shape)  # Should be [2, 10, 512]

class FeedForward(nn.Module):
    def __init__(self, embed_size, ff_hidden_size=2048, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(embed_size, ff_hidden_size),
            nn.ReLU(),
            nn.Linear(ff_hidden_size, embed_size),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.net(x)

class TransformerEncoderBlock(nn.Module):
    def __init__(self, embed_size, num_heads, ff_hidden_size, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadSelfAttention(embed_size, num_heads)
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        self.ff = FeedForward(embed_size, ff_hidden_size, dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # Attention + Add & Norm
        attn_output = self.attention(x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # Feedforward + Add & Norm
        ff_output = self.ff(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x

if __name__ == "__main__":
    block = TransformerEncoderBlock(embed_size=512, num_heads=8, ff_hidden_size=2048)
    x = torch.rand(2, 10, 512)
    out = block(x)
    print(out.shape)  # Should be [2, 10, 512]

class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_heads, ff_hidden_size, num_layers, max_len=100, dropout=0.1):
        super().__init__()
        self.token_embedding = TokenEmbedding(vocab_size, embed_size)
        self.pos_encoding = PositionalEncoding(embed_size, max_len)
        self.layers = nn.ModuleList([
            TransformerEncoderBlock(embed_size, num_heads, ff_hidden_size, dropout)
            for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        x = self.token_embedding(x)
        x = self.pos_encoding(x)
        x = self.dropout(x)

        for layer in self.layers:
            x = layer(x, mask)

        return x  # shape: [batch, seq_len, embed_size]

if __name__ == "__main__":
    vocab_size = 10000
    embed_size = 512
    encoder = TransformerEncoder(
        vocab_size=vocab_size,
        embed_size=embed_size,
        num_heads=8,
        ff_hidden_size=2048,
        num_layers=6,
        max_len=100
    )

    sample_input = torch.randint(0, vocab_size, (2, 20))  # batch=2, seq_len=20
    out = encoder(sample_input)
    print(out.shape)  # Should be [2, 20, 512]

class TransformerDecoderBlock(nn.Module):
    def __init__(self, embed_size, num_heads, ff_hidden_size, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadSelfAttention(embed_size, num_heads)
        self.norm1 = nn.LayerNorm(embed_size)

        self.cross_attn = MultiHeadSelfAttention(embed_size, num_heads)
        self.norm2 = nn.LayerNorm(embed_size)

        self.ff = FeedForward(embed_size, ff_hidden_size, dropout)
        self.norm3 = nn.LayerNorm(embed_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):
        # Masked self-attention
        _x = self.self_attn(x, tgt_mask)
        x = self.norm1(x + self.dropout(_x))

        # Cross-attention with encoder output
        _x = self.cross_attn(x= x, mask=memory_mask)  # simplified: you can later pass enc_out as key/value here
        x = self.norm2(x + self.dropout(_x))

        # Feedforward
        _x = self.ff(x)
        x = self.norm3(x + self.dropout(_x))
        return x
def generate_subsequent_mask(seq_len):
    mask = torch.tril(torch.ones((seq_len, seq_len))).unsqueeze(0).unsqueeze(0)
    return mask  # shape: [1, 1, seq_len, seq_len]

if __name__ == "__main__":
    decoder_block = TransformerDecoderBlock(embed_size=512, num_heads=8, ff_hidden_size=2048)
    x = torch.rand(2, 10, 512)
    enc_out = torch.rand(2, 20, 512)

    mask = generate_subsequent_mask(10)  # decoder input mask

    out = decoder_block(x, enc_out, tgt_mask=mask)
    print(out.shape)  # Should be [2, 10, 512]

class TransformerDecoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_heads, ff_hidden_size, num_layers, max_len=100, dropout=0.1):
        super().__init__()
        self.token_embedding = TokenEmbedding(vocab_size, embed_size)
        self.pos_encoding = PositionalEncoding(embed_size, max_len)
        self.layers = nn.ModuleList([
            TransformerDecoderBlock(embed_size, num_heads, ff_hidden_size, dropout)
            for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)
        self.fc_out = nn.Linear(embed_size, vocab_size)

    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):
        x = self.token_embedding(x)
        x = self.pos_encoding(x)
        x = self.dropout(x)

        for layer in self.layers:
            x = layer(x, enc_out, tgt_mask, memory_mask)

        return self.fc_out(x)  # logits over vocab

class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size=512, num_heads=8, ff_hidden_size=2048,
                 num_layers=6, max_len=100, dropout=0.1):
        super().__init__()
        self.encoder = TransformerEncoder(src_vocab_size, embed_size, num_heads, ff_hidden_size, num_layers, max_len, dropout)
        self.decoder = TransformerDecoder(tgt_vocab_size, embed_size, num_heads, ff_hidden_size, num_layers, max_len, dropout)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        enc_out = self.encoder(src, src_mask)
        out = self.decoder(tgt, enc_out, tgt_mask, memory_mask=src_mask)
        return out  # shape: [batch_size, tgt_seq_len, vocab_size]

if __name__ == "__main__":
    src_vocab_size = 10000
    tgt_vocab_size = 10000
    model = Transformer(src_vocab_size, tgt_vocab_size)

    src = torch.randint(0, src_vocab_size, (2, 20))  # [batch, src_seq_len]
    tgt = torch.randint(0, tgt_vocab_size, (2, 10))  # [batch, tgt_seq_len]

    tgt_mask = generate_subsequent_mask(tgt.size(1))  # causal mask

    out = model(src, tgt, tgt_mask=tgt_mask)
    print(out.shape)  # Should be [2, 10, 10000]

import torch
import torch.nn as nn
import math

# ------------------------------
# Embedding and Positional Encoding
# ------------------------------
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size, embed_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)

    def forward(self, x):
        return self.embedding(x)

class PositionalEncoding(nn.Module):
    def __init__(self, embed_size, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, embed_size)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

# ------------------------------
# Multi-Head Self Attention
# ------------------------------
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_size, num_heads):
        super().__init__()
        assert embed_size % num_heads == 0
        self.embed_size = embed_size
        self.num_heads = num_heads
        self.head_dim = embed_size // num_heads

        self.query = nn.Linear(embed_size, embed_size)
        self.key = nn.Linear(embed_size, embed_size)
        self.value = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)

    def forward(self, x, mask=None):
        N, seq_len, embed_size = x.shape

        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)

        def split_heads(tensor):
            return tensor.view(N, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        Q = split_heads(Q)
        K = split_heads(K)
        V = split_heads(V)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attention = torch.softmax(scores, dim=-1)
        out = torch.matmul(attention, V)
        out = out.transpose(1, 2).contiguous().view(N, seq_len, self.embed_size)
        return self.fc_out(out)

# ------------------------------
# Feed Forward Network
# ------------------------------
class FeedForward(nn.Module):
    def __init__(self, embed_size, ff_hidden_size=2048, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(embed_size, ff_hidden_size),
            nn.ReLU(),
            nn.Linear(ff_hidden_size, embed_size),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.net(x)

# ------------------------------
# Encoder Block
# ------------------------------
class TransformerEncoderBlock(nn.Module):
    def __init__(self, embed_size, num_heads, ff_hidden_size, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadSelfAttention(embed_size, num_heads)
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        self.ff = FeedForward(embed_size, ff_hidden_size, dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        x = self.norm1(x + self.dropout(self.attention(x, mask)))
        x = self.norm2(x + self.dropout(self.ff(x)))
        return x

# ------------------------------
# Full Encoder
# ------------------------------
class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_heads, ff_hidden_size, num_layers, max_len=100, dropout=0.1):
        super().__init__()
        self.token_embedding = TokenEmbedding(vocab_size, embed_size)
        self.pos_encoding = PositionalEncoding(embed_size, max_len)
        self.layers = nn.ModuleList([
            TransformerEncoderBlock(embed_size, num_heads, ff_hidden_size, dropout)
            for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        x = self.token_embedding(x)
        x = self.pos_encoding(x)
        x = self.dropout(x)

        for layer in self.layers:
            x = layer(x, mask)
        return x

# ------------------------------
# Decoder Block
# ------------------------------
class TransformerDecoderBlock(nn.Module):
    def __init__(self, embed_size, num_heads, ff_hidden_size, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadSelfAttention(embed_size, num_heads)
        self.norm1 = nn.LayerNorm(embed_size)

        self.cross_attn = MultiHeadSelfAttention(embed_size, num_heads)
        self.norm2 = nn.LayerNorm(embed_size)

        self.ff = FeedForward(embed_size, ff_hidden_size, dropout)
        self.norm3 = nn.LayerNorm(embed_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):
        x = self.norm1(x + self.dropout(self.self_attn(x, tgt_mask)))
        x = self.norm2(x + self.dropout(self.cross_attn(x= x, mask=memory_mask)))
        x = self.norm3(x + self.dropout(self.ff(x)))
        return x

# ------------------------------
# Full Decoder
# ------------------------------
class TransformerDecoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_heads, ff_hidden_size, num_layers, max_len=100, dropout=0.1):
        super().__init__()
        self.token_embedding = TokenEmbedding(vocab_size, embed_size)
        self.pos_encoding = PositionalEncoding(embed_size, max_len)
        self.layers = nn.ModuleList([
            TransformerDecoderBlock(embed_size, num_heads, ff_hidden_size, dropout)
            for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)
        self.fc_out = nn.Linear(embed_size, vocab_size)

    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):
        x = self.token_embedding(x)
        x = self.pos_encoding(x)
        x = self.dropout(x)

        for layer in self.layers:
            x = layer(x, enc_out, tgt_mask, memory_mask)
        return self.fc_out(x)

# ------------------------------
# Full Transformer Model
# ------------------------------
class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size=512, num_heads=8, ff_hidden_size=2048,
                 num_layers=6, max_len=100, dropout=0.1):
        super().__init__()
        self.encoder = TransformerEncoder(src_vocab_size, embed_size, num_heads, ff_hidden_size, num_layers, max_len, dropout)
        self.decoder = TransformerDecoder(tgt_vocab_size, embed_size, num_heads, ff_hidden_size, num_layers, max_len, dropout)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        enc_out = self.encoder(src, src_mask)
        out = self.decoder(tgt, enc_out, tgt_mask, memory_mask=src_mask)
        return out  # shape: [batch_size, tgt_seq_len, vocab_size]

# ------------------------------
# Mask Helper
# ------------------------------
def generate_subsequent_mask(seq_len):
    mask = torch.tril(torch.ones((seq_len, seq_len))).unsqueeze(0).unsqueeze(0)
    return mask  # shape: [1, 1, seq_len, seq_len]

# ------------------------------
# Test the Full Model
# ------------------------------
if __name__ == "__main__":
    src_vocab_size = 10000
    tgt_vocab_size = 10000
    model = Transformer(src_vocab_size, tgt_vocab_size)

    src = torch.randint(0, src_vocab_size, (2, 20))  # [batch, src_seq_len]
    tgt = torch.randint(0, tgt_vocab_size, (2, 10))  # [batch, tgt_seq_len]

    tgt_mask = generate_subsequent_mask(tgt.size(1))  # causal mask

    out = model(src, tgt, tgt_mask=tgt_mask)
    print("Transformer output shape:", out.shape)  # [2, 10, 10000]